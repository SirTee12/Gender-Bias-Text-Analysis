{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a7a4ee-8d58-46f3-ad0b-d79f48726c90",
   "metadata": {},
   "source": [
    "# Do students describe professors differently based on gender?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea79ab00-9335-48d5-9fa9-216776459af9",
   "metadata": {},
   "source": [
    "_Note: You can consult the solution of this live training in the file browser as `notebook-solution.ipynb`_\n",
    "\n",
    "Language plays a crucial role in shaping our perceptions and attitudes towards gender in the workplace, in classrooms, and personal relationships. Studies have shown that gender bias in language can have a significant impact on the way people are perceived and treated. \n",
    "\n",
    "For example, research has found that job advertisements that use masculine-coded language tend to attract more male applicants, while those that use feminine-coded language tend to attract more female applicants. Similarly, gendered language can perpetuate differences in the classroom.\n",
    "\n",
    "In this project, we'll using scraped student reviews from [ratemyprofessors.com](https://ratemyprofessors.com) to identify differences in language commonly used for male vs. female professors, and explore subtleties in how language in the classroom can be gendered.\n",
    "\n",
    "This excellent [tool](https://benschmidt.org/profGender/#%7B%22database%22%3A%22RMP%22%2C%22plotType%22%3A%22pointchart%22%2C%22method%22%3A%22return_json%22%2C%22search_limits%22%3A%7B%22word%22%3A%5B%22aggressive%22%5D%2C%22department__id%22%3A%7B%22%24lte%22%3A25%7D%2C%22rHelpful%22%3A%5B1%2C2%5D%2C%22rClarity%22%3A%5B1%2C2%5D%7D%2C%22aesthetic%22%3A%7B%22x%22%3A%22WordsPerMillion%22%2C%22y%22%3A%22department%22%2C%22color%22%3A%22gender%22%7D%2C%22counttype%22%3A%5B%22WordsPerMillion%22%5D%2C%22groups%22%3A%5B%22department%22%2C%22gender%22%5D%2C%22testGroup%22%3A%22C%22%7D) created by Ben Schmidt allows us to enter the words and phrases that we find in our analysis and explore them in more depth. We'll do this at the end.\n",
    "\n",
    "Catalyst also does some incredible work on [decoding](https://www.catalyst.org/2015/05/07/can-you-spot-the-gender-bias-in-this-job-description/) gendered language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba20f3ce-5247-49ba-9779-31786362fcbb",
   "metadata": {},
   "source": [
    "# 1. Scraping the web for reviews of professors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0f772-1772-43e5-a373-be16b17595dc",
   "metadata": {},
   "source": [
    "Text data‚Äì‚Äìespecially gendered text data, is hard to come by. Web scraping can be a helpful data collection tool when datasets are unable for this kind of work. We can write web scrapers to compile datasets on job descriptions, freelancer reviews, and, as in our use-case, professor reviews by students.\n",
    "\n",
    "[ratemyprofessors.com](https://www.ratemyprofessors.com/professor?tid=589) provides a wonderful combination of qualitative and quantitative metrics that we can analyze.\n",
    "\n",
    "Although the data on their websites is not labeled by gender, we'll use pronouns used by students to label professors \"Male\" or \"Female\". Of course, this approach is not perfect, as it relies on the _students'_ use of pronouns. Professors with non-binary pronouns will also be under-represented in the data, since very few reviews will have them, and so it's not trivial to write an algorithm to detect them. These are definitely important questions in the world of gender analysis though, so we encourage you to pick them up as extensions of this project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd150d6-5aae-4ebf-86f5-d8b398694e99",
   "metadata": {},
   "source": [
    "### Task 1a. What relevant packages do we need for web scraping and reading in data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d32b671-a5a9-4b1c-b730-dbd276406a93",
   "metadata": {
    "executionTime": 14,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# # Used to open urls\n# ____\n\n# # Used to parse html\n# ____\n\n# # Used to pause code intermittently so that our scraper is not blocked\n# ____\n\n# # For data manipulation and analysis\n# ____\n\n# # To access our data filenames so we can read them\n# ____"
   },
   "outputs": [],
   "source": [
    "# Used to open urls\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Used to parse html\n",
    "from lxml import etree\n",
    "\n",
    "# Used to pause code intermittently so that our scraper is not blocked\n",
    "import time\n",
    "\n",
    "# For data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# To access our data filenames so we can read them\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a97cd1a-5f8b-489b-b403-eb56cdffb44a",
   "metadata": {},
   "source": [
    "### Read the file \n",
    "\n",
    "Which professors will we be looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2fb34c-3bb1-4f97-91a6-dd4eb826d85b",
   "metadata": {},
   "source": [
    "The `web_scraping.ipynb` notebook provided in this workspace provides some code using selenium that was used to find urls from [ratemyprofessors.com](https://ratemyprofessors.com) that we'll be scraping in this notebook.\n",
    "\n",
    "For now, we'll open the file `profs_888.txt` and read each professor's url in a new line, and save this variable as `profs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b8efe1-4c75-4755-9534-a8889b82c6d6",
   "metadata": {
    "executionTime": 7,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# with open(r'profs_1244.txt', 'r') as f:\n#     profs = ____"
   },
   "outputs": [],
   "source": [
    "with open(r'profs_1244.txt', 'r') as f:\n",
    "    profs = [i.strip() for i in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0ffe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.ratemyprofessors.com/professor?tid=398',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=589',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=600',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=608',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=627',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=670',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=869',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=934',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=978',\n",
       " 'https://www.ratemyprofessors.com/professor?tid=1364']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify\n",
    "profs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802615b-401c-4c7a-b179-384d390ccbe1",
   "metadata": {},
   "source": [
    "###  How can we use urls to scrape relevant data about professors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e77b9-980a-494a-a652-a9f27c3a3659",
   "metadata": {},
   "source": [
    "Each professor has an overall rating that looks like this\n",
    "<img src=\"img/overall_rating_example.png\"  width=\"400\">\n",
    "\n",
    "and a series of reviews that look like this\n",
    "![Review example](img/review_example.png)\n",
    "\n",
    "The code below can be used to iterate through all or part of the list of urls in `profs`, and scrape them for qualtiative and quantitative data. **You won't need to run through this whole list though, because the `data/` folder already contains the reviews of several professors that we have scraped for you!**\n",
    "\n",
    "- The overall rating for the professor\n",
    "- All the individual reviews written by students about the professor\n",
    "- The \"emotion\" corresponding to each individual review: `üòé AWESOME`, `üòê AVERAGE`, or `üòñ AWFUL`\n",
    "- A numerical \"quality\" rating corresponding to each individual review\n",
    "\n",
    "We won't be using the \"difficulty\" ratings shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432d6789-8153-4de1-baae-ae211ca49c23",
   "metadata": {
    "executionTime": 92795,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# USE ONLY ONE OF THE FOLLOWING FOR STATWEMENTS\n\n# 1. Sample code to loop through the whole list of professors    \n# for s in (range(40, len(profs),10)):\n\n# 2. Sample code to loop through the first 10 professors\nfor s in (range(0, 10, 10)):\n\n    texts = [] # Initialzie an empty array\n    print((s, s+10)) # Iterate through 10 professors at a time\n    \n    for url in profs[s:s+10]: # Iterate through this block\n        time.sleep(8) # To prevent sending too many requests at once\n        r = urlopen(url) # Open URL\n        htmlparser = etree.HTMLParser() # Instantiate a parser to parse HTML\n        tree = etree.parse(r, htmlparser) # Parse HTML returned by the url\n        \n        text = tree.xpath('//*[@id=\"ratingsList\"]/li[*]/div/div/div[3]/div[3]/text()') # Extract reviews\n        ratings = tree.xpath('//*[@id=\"root\"]/div/div/div[3]/div[2]/div[1]/div[1]/div[1]/div/div[1]') # Extract ratings\n        emotion = tree.xpath('//*[@id=\"ratingsList\"]/li[*]/div/div/div[1]/div[1]/div[2]/text()') # Extract emotion\n        quality = tree.xpath('//*[@id=\"ratingsList\"]/li[*]/div/div/div[2]/div[1]/div/div[2]') # Extract quality\n        texts.append((url,\n                      text,\n                      [i.text for i in ratings][0],\n                      emotion,\n                      [i.text for i in quality],\n                     )) # Append metrics to empty list\n\n    print() # Print new line for readability\n    df = pd.DataFrame(texts, columns = ['prof','review', 'rating','emotion','quality']) # Create DataFrame from texts\n    df.to_csv(f'df_{s}_to_{s+10}.csv') # Write result to df in blocks of 10 professors at a time\n    time.sleep(10) # Pause to prevent sending too many requests at once",
    "outputsMetadata": {
     "0": {
      "height": 60,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USE ONLY ONE OF THE FOLLOWING FOR STATWEMENTS\n",
    "\n",
    "# 1. Sample code to loop through the whole list of professors    \n",
    "# for s in (range(40, len(profs),10)):\n",
    "\n",
    "# 2. Sample code to loop through the first 10 professors\n",
    "for s in range(0, 10, 10):\n",
    "\n",
    "    texts = [] # Initialzie an empty array\n",
    "    print((s, s+10)) # Iterate through 10 professors at a time\n",
    "    \n",
    "    for url in profs[s:s+10]: # Iterate through this block\n",
    "        time.sleep(8) # To prevent sending too many requests at once\n",
    "        r = urlopen(url) # Open URL\n",
    "        htmlparser = etree.HTMLParser() # Instantiate a parser to parse HTML\n",
    "        tree = etree.parse(r, htmlparser) # Parse HTML returned by the url\n",
    "        \n",
    "        text = tree.xpath('//*[@id=\"ratingsList\"]/li[*]/div/div/div[3]/div[3]/text()') # Extract reviews\n",
    "        ratings = tree.xpath('//*[@id=\"root\"]/div/div/div[3]/div[2]/div[1]/div[1]/div[1]/div/div[1]') # Extract ratings\n",
    "        emotion = tree.xpath('//*[@id=\"ratingsList\"]/li[*]/div/div/div[1]/div[1]/div[2]/text()') # Extract emotion\n",
    "        quality = tree.xpath('//*[@id=\"ratingsList\"]/li[*]/div/div/div[2]/div[1]/div/div[2]') # Extract quality\n",
    "        texts.append((url,\n",
    "                      text,\n",
    "                      [i.text for i in ratings][0],\n",
    "                      emotion,\n",
    "                      [i.text for i in quality],\n",
    "                     )) # Append metrics to empty list\n",
    "\n",
    "    print() # Print new line for readability\n",
    "    df = pd.DataFrame(texts, columns=['professor', 'reviews', \n",
    "                                     'rating', 'emotion', 'quality'])\n",
    "    df.to_csv(f'df_{s}_to_{s+10}.csv') # Write result to df in blocks of 10 professors at a time\n",
    "    time.sleep(10)# Pause to prevent sending too many requests at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e3b3a-1673-497d-bff8-dca84520018a",
   "metadata": {},
   "source": [
    "# 2. Reading pre-scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b0595-0ccd-47e0-9a20-adda21d7a769",
   "metadata": {},
   "source": [
    "### Task 2a. How can we read a directory of scraped professor reviews and concatenate them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fe127-441b-4453-ac3b-569b24cd85d5",
   "metadata": {},
   "source": [
    "Since we have already scraped reviews from several professors for you, let's begin by concatenating all the files in the `data` folder provided. These have already been scraped for you.\n",
    "\n",
    "Since `review`, `emotion` and `quality` are lists but were recorded in string form, we'll apply `eval()` to them to turn them back from a string into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0eb371-f339-4c0a-b3e2-0a5840af2706",
   "metadata": {
    "executionTime": 11,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# df = ____\n# df['review'] = ____\n# df['emotion'] = df[____].apply(lambda x: eval(x))\n# df['quality'] = df[____].apply(lambda x: eval(x))"
   },
   "outputs": [],
   "source": [
    "prof_review = pd.concat([pd.read_csv('data/'+i, index_col=0) for i in os.listdir('data')]).reset_index(drop= True)\n",
    "prof_review['review'] = prof_review['review'].apply(lambda x: eval(x))\n",
    "prof_review['emotion'] = prof_review['emotion'].apply(lambda x: eval(x))\n",
    "prof_review['quality'] = prof_review['quality'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700fde1-0644-4208-8bd5-f2272482d31e",
   "metadata": {},
   "source": [
    "### Task 2b. What does the final shape of our DataFrame look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b377de-594c-46c2-8bd8-4af9a0eddc00",
   "metadata": {},
   "source": [
    "Browse the `df` below to familiarize yourself with the dataset we'll be working with. The DataFrame contains one row for each professor, containing:\n",
    "- Their url\n",
    "- All the raw text reviews for that professor\n",
    "- Their overall rating\n",
    "- All the emotion labels associated with reviews of that professor\n",
    "- All quality ratings assigned to that professor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33dbb820-d76a-4607-96f3-0ff3859c0e6b",
   "metadata": {
    "executionTime": 101,
    "lastSuccessfullyExecutedCode": "# df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prof</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>emotion</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.ratemyprofessors.com/professor?tid...</td>\n",
       "      <td>[I liked his class. Sure he can be a little bo...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>[awesome, awful, average, awesome, awesome, aw...</td>\n",
       "      <td>[4.0, 2.5, 3.0, 5.0, 5.0, 4.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.ratemyprofessors.com/professor?tid...</td>\n",
       "      <td>[Dr. Gao is very knowledgeable.  His enthusias...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>[average, awesome, awful, awful, average, awful]</td>\n",
       "      <td>[3.5, 4.0, 1.0, 2.5, 3.5, 2.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.ratemyprofessors.com/professor?tid...</td>\n",
       "      <td>[Tests are way too hard for a course that shou...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>[awful, average, awful, awful, average, awful,...</td>\n",
       "      <td>[1.0, 3.0, 2.5, 2.5, 3.5, 1.5, 3.5, 5.0, 3.5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.ratemyprofessors.com/professor?tid...</td>\n",
       "      <td>[He is one of the best professor when it comes...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>[awesome, awesome, awesome, awesome, awesome, ...</td>\n",
       "      <td>[5.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, 1.0, 1.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.ratemyprofessors.com/professor?tid...</td>\n",
       "      <td>[She is the worst, and she makes you not want ...</td>\n",
       "      <td>3.3</td>\n",
       "      <td>[awful, awful, average, awful, average, awful,...</td>\n",
       "      <td>[1.0, 2.5, 3.0, 2.5, 3.5, 1.0, 3.0, 5.0, 2.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prof  \\\n",
       "0  https://www.ratemyprofessors.com/professor?tid...   \n",
       "1  https://www.ratemyprofessors.com/professor?tid...   \n",
       "2  https://www.ratemyprofessors.com/professor?tid...   \n",
       "3  https://www.ratemyprofessors.com/professor?tid...   \n",
       "4  https://www.ratemyprofessors.com/professor?tid...   \n",
       "\n",
       "                                              review  rating  \\\n",
       "0  [I liked his class. Sure he can be a little bo...     4.0   \n",
       "1  [Dr. Gao is very knowledgeable.  His enthusias...     2.8   \n",
       "2  [Tests are way too hard for a course that shou...     3.6   \n",
       "3  [He is one of the best professor when it comes...     3.8   \n",
       "4  [She is the worst, and she makes you not want ...     3.3   \n",
       "\n",
       "                                             emotion  \\\n",
       "0  [awesome, awful, average, awesome, awesome, aw...   \n",
       "1   [average, awesome, awful, awful, average, awful]   \n",
       "2  [awful, average, awful, awful, average, awful,...   \n",
       "3  [awesome, awesome, awesome, awesome, awesome, ...   \n",
       "4  [awful, awful, average, awful, average, awful,...   \n",
       "\n",
       "                                             quality  \n",
       "0                     [4.0, 2.5, 3.0, 5.0, 5.0, 4.5]  \n",
       "1                     [3.5, 4.0, 1.0, 2.5, 3.5, 2.5]  \n",
       "2  [1.0, 3.0, 2.5, 2.5, 3.5, 1.5, 3.5, 5.0, 3.5, ...  \n",
       "3  [5.0, 4.0, 5.0, 5.0, 5.0, 4.0, 3.0, 1.0, 1.0, ...  \n",
       "4  [1.0, 2.5, 3.0, 2.5, 3.5, 1.0, 3.0, 5.0, 2.0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof_review.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c95fb-8fa5-4278-9401-55132918db3e",
   "metadata": {},
   "source": [
    "# 3. Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f784a6d5-7980-494d-a169-00579322eeae",
   "metadata": {},
   "source": [
    "## Additional package imports are required for data visualization and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d6a7a01-887d-4df1-98b6-5a3c95e35519",
   "metadata": {
    "executionTime": 7,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "import numpy as np\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\n\nfrom nltk.tokenize import WhitespaceTokenizer\n# nltk.download('punkt')\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.feature_extraction import text\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='white')\n\n# from scipy import stats"
   },
   "outputs": [],
   "source": [
    "import numpy as np # For manipulating matrices during NLP\n",
    "\n",
    "import nltk # Natural language toolkit\n",
    "from nltk.tokenize import word_tokenize # Used for breaking up strings of text (e.g. sentences) into words\n",
    "from  nltk.stem.porter import PorterStemmer # Used to return the dictionary base of a word\n",
    "from nltk.tokenize import WhitespaceTokenizer # Used for breaking up strings of text (e.g. sentences) into words based on white space\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text # Using to extrat features from text\n",
    "# Used to count the occurences of words and phrases\n",
    " # Using to extrat features from text\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f712991-c01a-40b1-93e0-34d3f22969b3",
   "metadata": {},
   "source": [
    "### 3b. How can we assign gender labels to professors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75750667-fe53-4a54-a79d-4ae4432c3e08",
   "metadata": {},
   "source": [
    "Let's write a custom function that assigns a gender label to professors based on the pronouns most commontly used for him. Specifically:\n",
    "- If any of `['she', 'her', 'herself', 'shes']` occur more than 5 times across all reviews for that professor, we label the professor \"F\".\n",
    "- If any of `['him', 'he', 'his', 'himself']` occur more than 5 times across all reviews for that professor, we label the professor \"F\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b44c0b61-91e6-4a2e-b969-8e12d9dca25e",
   "metadata": {
    "executionTime": 4,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "def assign_pronoun(review_list):\n    reviews = ' '.join([str(i) for i in review_list])\n    tokens = word_tokenize(reviews.lower())\n    if sum([i in ['she', 'her', 'herself'] for i in tokens]) > 5:\n        return 'F'\n    elif sum([i in ['him', 'he', 'his', 'himself'] for i in tokens]) > 5:\n        return 'M'"
   },
   "outputs": [],
   "source": [
    "def assign_pronoun(review_list):\n",
    "    ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ceafc103-6ee0-4d16-a61c-aadbf489900c",
   "metadata": {
    "executionTime": 5578,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "df['pronouns'] = df['review'].apply(assign_pronoun)\ndf['review_tokens'] = df['review'].apply(lambda x: word_tokenize(' '.join([str(i) for i in x])))"
   },
   "outputs": [],
   "source": [
    "df['pronouns'] = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5855d24-939a-4b8e-8fa5-43d33484e215",
   "metadata": {},
   "source": [
    "### 3c. Are there any initial differences between male and female professors based on their overall ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ebb76-8772-4a96-9568-29b6d4b31854",
   "metadata": {},
   "source": [
    "Let's start with a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d839f267-6424-4f91-9753-6f14741947e8",
   "metadata": {
    "executionTime": 12,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# plt.figure(figsize=(____))\n# ____\n# plt.show()"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(____))\n",
    "____\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d7abd-6bfb-4331-b302-aae759c825b6",
   "metadata": {},
   "source": [
    "A boxplot overlaid with a stripplot will give us a better sense of the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "171c79c8-f16b-43c7-9692-5123a609d72e",
   "metadata": {
    "executionTime": 18,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# plt.figure(figsize=(5,5))\n# ____\n# ____\n# plt.show()"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "____\n",
    "____\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5534f-7399-4c52-9544-d1f82b100f44",
   "metadata": {},
   "source": [
    "## Task 3d. What are the most important words being used to describe professors in reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0a53e-8354-4335-9ce7-ff4ca427dc26",
   "metadata": {},
   "source": [
    "Let's write a custom function that **tokenizes** and **lemmatizes** our list of words.\n",
    "- **Word tokenization**: process of splitting text into individual words, called tokens. A common preprocessing step in natural language processing (NLP) so that text can be analyzed and processed more easily. Methods include whitespace tokenization, regular expression-based tokenization, and rule-based tokenization. We'll be using the `word_tokenize` tokenizer from `nltk`, with all its defaults.\n",
    "- **Lemmatization**: process of reducing words to their base or dictionary form, called the lemma. Also a common pre-processing step in NLP, so that words with a common base form are treated the same way. For example, the lemma of \"am\" is \"be\", of \"running\" is \"run\", and of \"mice\" is \"mouse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "14af1c53-1fae-4673-b8a9-c80945fcf6b0",
   "metadata": {
    "executionTime": 8,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "def tokenize(text):\n    tk = WhitespaceTokenizer()\n    tokens = tk.tokenize(text)\n    stems = []\n    for item in tokens:\n        stems.append(PorterStemmer().stem(item))\n    return stems\n    # return tokens"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tk = WhitespaceTokenizer()\n",
    "    tokens = tk.tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e5a342-e573-4989-9139-9da1125c7612",
   "metadata": {},
   "source": [
    "Let's import a list of stop words, which are common English words that we will be ignoring in our analysis. `sklearn` provides a common list of stop words, and we can append additional words to this list. Below, we append pronouns, along with the words \"class\" and \"student\". Feel free to add any additional words you'd like to ignore to this list later on as you try to build upon this analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17fee0ae-b095-4132-b09b-f06bf8dda71b",
   "metadata": {
    "executionTime": 6,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",\n                                              \"himself\",\"herself\", \"hers\",\"shes\"\n                                              \"class\",\"student\"])"
   },
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"he\",\"she\",\"his\",\"her\",\n",
    "                                              \"himself\",\"herself\", \"hers\",\"shes\"\n",
    "                                              \"class\",\"student\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24febc-27d7-4c84-910c-9118ea6832fa",
   "metadata": {},
   "source": [
    "For the purpose of analyzing review texts, we want to move from having one row for each professor to one row for each review. Lets do this with `.explode()` from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91f24d99-a96d-4a6b-8f19-b972aac0885f",
   "metadata": {
    "executionTime": 16,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "df_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]\nq = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()\nq['quality'] = q['quality'].astype(float)"
   },
   "outputs": [],
   "source": [
    "df_quality = df[(df['review'].apply(len) == df['quality'].apply(len))]\n",
    "q = df_quality[['pronouns','review','quality']].explode(['review','quality'], ignore_index=True).dropna()\n",
    "q['quality'] = q['quality'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e3031-d6d0-46e1-8494-2ecc452366a2",
   "metadata": {},
   "source": [
    "TFIDF vectorization is the process of assigning scores to each review in a document based on how frequently the word occurs, normalized by how frequently the word occurs in the dataset overall.\n",
    "\n",
    "We'll use `TfidfVectorizer()` to generate these scores. This will return a matrix, with as many rows as reviews, and as many columns as words in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ff1b94d-755e-4777-a149-57af7c806207",
   "metadata": {
    "executionTime": 9477,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "vec = TfidfVectorizer(tokenizer=tokenize, stop_words=my_stop_words,\n                     ngram_range=(1,4))\nX = vec.fit_transform(q['review'])\nfeature_names = vec.get_feature_names_out()"
   },
   "outputs": [],
   "source": [
    "vec = ____\n",
    "X = ____\n",
    "feature_names = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a9c568-528c-40b2-b223-0ab06323bddb",
   "metadata": {},
   "source": [
    "`X` is a sparse matrix. We'll now move into filtering X for:\n",
    "- Rows with male professors and reviews of high quality \n",
    "- Rows with female professors and reviews of high quality \n",
    "- Rows with male professors and reviews of low quality \n",
    "- Rows with female professors and reviews of low quality \n",
    "\n",
    "We can explore feature importance in each of these to get a sense of which words and phrases are coming up most often in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "28537a88-8a7e-42c4-ba8b-276532defc87",
   "metadata": {
    "executionTime": 0,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "Xm = X[q['pronouns']=='M',:]\nXf = X[q['pronouns']=='F',:]\nm_pos = X[(q['pronouns']=='M') & (q['quality']>=4.5),:] \nf_pos = X[(q['pronouns']=='F') & (q['quality']>=4.5),:] \nm_neg = X[(q['pronouns']=='M') & (q['quality']<2.5),:] \nf_neg = X[(q['pronouns']=='M') & (q['quality']<2.5),:] "
   },
   "outputs": [],
   "source": [
    "m_pos = ____\n",
    "f_pos = ____\n",
    "m_neg = ____\n",
    "f_neg = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d09c0-5268-43a0-bfc6-2a1b610b864d",
   "metadata": {},
   "source": [
    "Let's have a look at what language students are using to describe male professors positively. The code below will return the 300 most important ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f03fbae-4024-4c85-b9a5-7924d06af553",
   "metadata": {
    "executionTime": 40,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "# importance = ____\n# tfidf_feature_names = ____\n# tfidf_feature_names[importance[:300]]"
   },
   "outputs": [],
   "source": [
    "importance = ____\n",
    "tfidf_feature_names = ____\n",
    "tfidf_feature_names[importance[:300]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07291053-814e-4b40-acdf-e3e415cebef0",
   "metadata": {},
   "source": [
    "Let's have a look at what language students are using to describe female professors positively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc8b08-e149-4938-a490-658e336ef797",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "importance = ____\n",
    "tfidf_feature_names = ____\n",
    "tfidf_feature_names[importance[:300]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967b01c-1f56-4fa2-ba7b-300d2dcaeed1",
   "metadata": {},
   "source": [
    "Let's have a look at what language students are using to describe male professors negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcc7dfc9-edab-4d8c-9a37-d02aa3549ec7",
   "metadata": {
    "executionTime": 11,
    "lastSuccessfullyExecutedCode": "# importance = ____\n# tfidf_feature_names = ____\n# tfidf_feature_names[importance[:300]]"
   },
   "outputs": [],
   "source": [
    "importance = ____\n",
    "tfidf_feature_names = ____\n",
    "tfidf_feature_names[importance[:300]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2dc505-ebab-42d4-800e-df381d02ebe4",
   "metadata": {},
   "source": [
    "Let's have a look at what language students are using to describe female professors positively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a970b19e-371d-4e4e-b0e3-b1f4529c40bc",
   "metadata": {
    "executionTime": 15,
    "lastSuccessfullyExecutedCode": "# importance = ____\n# tfidf_feature_names = ____\n# tfidf_feature_names[importance[:300]]"
   },
   "outputs": [],
   "source": [
    "importance = ____\n",
    "tfidf_feature_names = ____\n",
    "tfidf_feature_names[importance[:300]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ffc57-0591-4762-9d64-1c1bb571fc78",
   "metadata": {},
   "source": [
    "## Congratulations on making it to the end! \n",
    "### Where to from here?\n",
    "- We can feed these words into Ben Schmidt's [tool](https://benschmidt.org/profGender/#%7B%22database%22%3A%22RMP%22%2C%22plotType%22%3A%22pointchart%22%2C%22method%22%3A%22return_json%22%2C%22search_limits%22%3A%7B%22word%22%3A%5B%22his%20kids%22%2C%22her%20kids%22%5D%2C%22department__id%22%3A%7B%22%24lte%22%3A25%7D%7D%2C%22aesthetic%22%3A%7B%22x%22%3A%22WordsPerMillion%22%2C%22y%22%3A%22department%22%2C%22color%22%3A%22gender%22%7D%2C%22counttype%22%3A%5B%22WordCount%22%2C%22TotalWords%22%5D%2C%22groups%22%3A%5B%22unigram%22%5D%2C%22testGroup%22%3A%22C%22%7D) to derive insights by field.\n",
    "- If you're interested in learning more about [web scraping](https://app.datacamp.com/learn/courses/web-scraping-with-python), take our courses on Web Scraping in Python\n",
    "- If you're intersted in diving in to the world of Natural Language Processing, explore our [skill track](https://app.datacamp.com/learn/skill-tracks/natural-language-processing-in-python)."
   ]
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
